{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [path.split('xml\\\\')[1].split('.xml')[0] for path in glob('xml/*.xml')]\n",
    "books = ['MAT', 'MAR', 'LUK', 'JOH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in []:#langs:\n",
    "    print(lang)\n",
    "    root = ET.fromstring(open(f'xml/{lang}.xml', encoding='utf-8').read())\n",
    "    with open(f'txt/{lang}.txt', 'w', encoding='utf-8') as out:        \n",
    "        for book in books:\n",
    "            for seg in root.findall(f'.//div[@id=\"b.{book}\"]/*seg'):             \n",
    "                out.write(\"\".join(seg.itertext()).strip(\" 1234567890-\").strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in []:#langs:\n",
    "    print(lang)\n",
    "    ids = defaultdict(count().__next__)\n",
    "    edges = set()\n",
    "    words = []\n",
    "    with open(f'txt/{lang}.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words.extend(re.sub('[^\\w ]', '', line.lower().replace('&quot;', '')).split(' '))    \n",
    "    for i, j in zip(words, words[1:]):\n",
    "        source, target = ids[i], ids[j]\n",
    "        if ((source, target) not in edges) and ((target, source) not in edges):\n",
    "            edges.add((source, target))\n",
    "    g = ig.Graph()\n",
    "    g.add_edges(edges)\n",
    "    g.save(f'net/{lang}.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"vcount\": ig.Graph.vcount,\n",
    "    \"ecount\": ig.Graph.ecount,\n",
    "    \"density\": ig.Graph.density,\n",
    "    \"transitivity\": ig.Graph.transitivity_undirected,\n",
    "    \"assortativity_degree\": ig.Graph.assortativity_degree,\n",
    "    \"transitivity_avglocal\": ig.Graph.transitivity_avglocal_undirected,    \n",
    "    # Slow\n",
    "    # ig.Graph.average_path_length # 9s\n",
    "}\n",
    "meanMetrics = {\n",
    "    \"degree\": ig.Graph.degree,    \n",
    "    \"pagerank\": ig.Graph.pagerank,\n",
    "    \"coreness\": ig.Graph.coreness,\n",
    "    \"hub_score\": ig.Graph.hub_score,    \n",
    "    \"constraint\": ig.Graph.constraint,\n",
    "    \"feedback_arc_set\": ig.Graph.feedback_arc_set,\n",
    "    # Duplicated\n",
    "    # ig.Graph.strength,\n",
    "    # ig.Graph.authority_score,\n",
    "    # ig.Graph.personalized_pagerank,\n",
    "    # ig.Graph.eigenvector_centrality,\n",
    "    # Slow\n",
    "    # ig.Graph.closeness,\n",
    "    # ig.Graph.betweenness,\n",
    "    # ig.Graph.eccentricity,\n",
    "    # ig.Graph.similarity_dice,\n",
    "    # ig.Graph.edge_betweenness,\n",
    "    # ig.Graph.similarity_jaccard,\n",
    "    # ig.Graph.similarity_inverse_log_weighted,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for lang in []:#langs:    \n",
    "    print(lang)\n",
    "    row = {}\n",
    "    g = ig.load(f'net\\\\{lang}.net') \n",
    "    for name, metric in metrics.items():\n",
    "        row[name] = metric(g)\n",
    "    for name, metric in meanMetrics.items():\n",
    "        row[name] = np.mean(metric(g))\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows, langs)\n",
    "# df.to_csv('langs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('langs.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['subgenus'] = df['genus']+df['subgenus']\n",
    "X = df.drop(columns=['abbreviation', 'genus', 'subgenus'])\n",
    "\n",
    "print('Frequency', dict(Counter(df['genus']).most_common()))\n",
    "y = df['genus'].astype('category').cat.codes.values\n",
    "print('Mapping', dict(zip(df['genus'], y)))\n",
    "\n",
    "print('Frequency', dict(Counter(df['subgenus']).most_common()))\n",
    "y = df['subgenus'].astype('category').cat.codes.values\n",
    "print('Mapping', dict(zip(df['subgenus'], y)))\n",
    "#y = (df['genus']+df['subgenus']).astype('category').cat.codes.values\n",
    "#dict(zip(y, df['genus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n",
    "X2 = lda.fit_transform(X, y)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "#print(metrics.silhouette_score(X2, y, random_state=0)) # higher is better\n",
    "#print(metrics.calinski_harabasz_score(X2, y)) # higher is better\n",
    "#print(metrics.davies_bouldin_score(X2, y)) # lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda.explained_variance_ratio_\n",
    "#lda.coef_\n",
    "#lda.means_\n",
    "#lda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=0)\n",
    "X2 = nca.fit_transform(X, y)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "#print(metrics.silhouette_score(X2, y, random_state=0)) # higher is better\n",
    "#print(metrics.calinski_harabasz_score(X2, y)) # higher is better\n",
    "#print(metrics.davies_bouldin_score(X2, y)) # lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "\n",
    "y = df['genus'].astype('category').cat.codes.values\n",
    "y_pred = []\n",
    "for train_index, test_index in LeaveOneOut().split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train, y_train = SMOTE(random_state=1, k_neighbors=1, n_jobs=-1).fit_resample(X_train, y_train)    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        dimRed = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n",
    "        X_train = dimRed.fit_transform(X_train, y_train)                \n",
    "        predicted = dimRed.predict(X_test)\n",
    "        #X_test = dimRed.transform(X_test)    \n",
    "    y_pred.append(predicted)\n",
    "    '''\n",
    "    if predicted!=y_test:\n",
    "        print(X_test, y_test, predicted)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='red')\n",
    "        plt.show() \n",
    "    '''    \n",
    "print(np.mean([i==j for i,j in zip(y, y_pred)]))\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subgenus'].astype('category').cat.codes.values\n",
    "y_pred = []\n",
    "for train_index, test_index in LeaveOneOut().split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        dimRed = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto', n_components=4)\n",
    "        X_train = dimRed.fit_transform(X_train, y_train)                \n",
    "        X_test = dimRed.transform(X_test)\n",
    "    clf = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)    \n",
    "    y_pred.append(predicted)    \n",
    "print(np.mean([i==j for i,j in zip(y, y_pred)]))\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
